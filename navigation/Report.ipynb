{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we train an agent to navigate a large, square world and collect bananas. \n",
    "\n",
    "![Banana](https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif)\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  \n",
    "As such, the goal of the agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's \n",
    "forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:\n",
    "\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agent, we use the Deep Q-Learning algorithm with experience replay introduced by Google's DeepMind in [this](https://www.nature.com/articles/nature14236) paper.  \n",
    "\n",
    "To train an agent using deep reinforcement learning, we need the following components:\n",
    "\n",
    "- **Environment**: The environment here is provided to us using Unity ML. To interact/integrate with the environment, we need to get familiar with the python API. This has been explained in the Navigation.ipynb.\n",
    "\n",
    "\n",
    "- **Agent**: This is the player in the Banana environment that we control. The agent should be able to take an action given a state (using epsilon-greedy algorithm) and also learn as it interacts with the environment. The agent has been implemented using the class \"Agent\" in the \"dqn_agent.py\" module. To train the agent, we use the following hyperparameters: \n",
    "\n",
    "    + BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    + BATCH_SIZE = 64         # minibatch size\n",
    "    + GAMMA = 0.99            # discount factor\n",
    "    + TAU = 1e-3              # for soft update of target parameters\n",
    "    + LR = 5e-4               # learning rate \n",
    "    + UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "\n",
    "- **Action-value function and target action-value function**: These are our deep neural networks that translate a state to a vector indicating the value of each action. Both networks will have identical archituctures. The deep learning model has been implemented in the model.py module. Here, we are using a relatively simple architucture with two fully connected hidden layers (each with 64 neurons) and relu activation functions. This architucture seems to be sufficient for this environment.\n",
    "\n",
    "\n",
    "- **Replay memory**: This is an array of a fixed length (we use double-ended queue here) that holds the state/action/reward/next_state information used during learning process. The replay memory is implemented using the class \"ReplayBuffer\" in the \"dqn_agent.py\" module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agent, we have implemented the function \"dqn\" in the Navigation.ipynb. This function trains the agent for a number of episodes and keeps track of the scores. We stop the training process once we achieve an average score of +13 over 100 consecutive episodes. \n",
    "\n",
    "As it can be seen in the Navigation.ipynb, the agent achieves an average score of 13+ in less than 400 episodes (365 to be more precise), which is relatively quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the introduction of the Deep Q-Learning algorithm in 2015, several imrovements have been suggested:\n",
    "    \n",
    "- **Prioritized Experience Replay**\n",
    "- **Double DQN**\n",
    "- **Dueling DQN**\n",
    "\n",
    "One can try to implement these algorithms to improve the performance of the agent here. An interesting approach would be training the agent using only the images. \n",
    "\n",
    "Given that the agent currently trains quite quickly with a relatively good performance, those algorithms may not be needed. For example, I increased the threshold to 20 and trained the agent for ~2000 episodes. The agent was able to achieve a much better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
